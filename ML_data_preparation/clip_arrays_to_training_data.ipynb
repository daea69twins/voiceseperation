{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b874b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZaknafeinII\\anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "To use this script, make the following preparations:\n",
    "- Create a working directory\n",
    "- Inside the working directory, have a single ..._clips.npy file. This file will correspond to which\n",
    "  character's model the generated data will be used to train.\n",
    "- Inside the working directory, have a single folder named 'overlay_clips'\n",
    "- Inside the overlay_clips directory, include one or more ..._clips.npy files of other characters. This will\n",
    "  be used generate data for the known character's model to differentiate from other characters.\n",
    "- Inside the working directory, create an empty subdirectory named 'output'\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pydub\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a77966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify script parameters\n",
    "\"\"\"\n",
    "# specify the working directory; note that directory notation must use '/' rather than '\\'\n",
    "wd = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/'\n",
    "\n",
    "# initially using 70%/15%/15% proportions for training, validation, and testing\n",
    "train_proportion = 0.7\n",
    "val_proportion = 0.15\n",
    "\n",
    "# number of MFCC features\n",
    "# 20 is the default for the librosa .wav->MFCC feature extraction\n",
    "# 39 seems to be normal for speech recognition (but not reconstruction)\n",
    "# 64 seems to be the minimum for good speech reconstruction, but the reconstruction can be lengthy\n",
    "n_mfcc = 64\n",
    "\n",
    "# MFCC framing of audio frames\n",
    "# 2048 and 512 are the default values, respectively\n",
    "win_length=2048\n",
    "hop_length=512\n",
    "\n",
    "# the sampling rate to use; default is 22050\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dca2934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For known character data: id10016_merged_cleaned.wav_clips.npy\n",
      "\n",
      "For other character data:\n",
      "\tid10130_merged_cleaned.wav_clips.npy\n",
      "\tid10168_merged_cleaned.wav_clips.npy\n",
      "\tid10484_merged_cleaned.wav_clips.npy\n"
     ]
    }
   ],
   "source": [
    "# this will store the known character's clips\n",
    "known_char_clips = None\n",
    "\n",
    "# this will store the clips for all other characters\n",
    "other_char_clips_array = []\n",
    "\n",
    "# navigate the specified working directory to determine known character and other character clips arrays\n",
    "wd_contents = os.listdir(wd)\n",
    "wd_overlays_contents = os.listdir(wd+'overlay_clips')\n",
    "\n",
    "for item in wd_contents:\n",
    "    if os.path.isfile(os.path.join(wd, item)):\n",
    "        print('For known character data: %s\\n' % (item))\n",
    "        \n",
    "        # load the known character's ...clips.npy file\n",
    "        with open(wd+item, 'rb') as f:\n",
    "            known_char_clips = np.load(f)\n",
    "            \n",
    "print('For other character data:')\n",
    "for item in wd_overlays_contents:\n",
    "    # load the current other character's ...clips.npy file\n",
    "        with open(wd+'overlay_clips/'+item, 'rb') as f:\n",
    "            other_char_clips_array.append(np.load(f))\n",
    "            \n",
    "        print('\\t%s' % (item))\n",
    "    \n",
    "            \n",
    "other_char_clips_array = np.array(other_char_clips_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4646ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known_char_clips.shape: (1497, 88200)\n",
      "other_char_clips_array.shape: (3, 1497, 88200)\n"
     ]
    }
   ],
   "source": [
    "print('known_char_clips.shape:', known_char_clips.shape)\n",
    "print('other_char_clips_array.shape:', other_char_clips_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6a3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffle all clips for all characters and set up the clips to be overlayed.\n",
    "Because clip overlaying is based on averaging values, overlaying a known char clip\n",
    "with itself returns itself.\n",
    "\n",
    "There are n+1 overlay options that are selected randomly, where n is the number of\n",
    "other characters to choose from. The +1 is the choice of no overlay (by overlaying with\n",
    "itself).\n",
    "\"\"\"\n",
    "\n",
    "# we need to shuffle order of the clips\n",
    "# shuffle known character\n",
    "random.shuffle(known_char_clips)\n",
    "# shuffle other characters\n",
    "for index in range(0, len(other_char_clips_array)):\n",
    "    random.shuffle(other_char_clips_array[index])\n",
    "    \n",
    "\n",
    "# overlay_clips stores the clips to overlay the known character\n",
    "overlay_clips = []\n",
    "\n",
    "# randomly select clips from the other characters to append to overlay clips\n",
    "# can also do the same clip as the known character (which represents no overlay, just the normal voice)\n",
    "# all scenarios have equal chance\n",
    "while len(overlay_clips) < len(known_char_clips):\n",
    "    # determine random index\n",
    "    # randint() is inclusive; if the int is out of bounds we interpret this as no overlay\n",
    "    rand_index = random.randint(0, len(other_char_clips_array))\n",
    "    \n",
    "    # no overlay\n",
    "    if rand_index == len(other_char_clips_array):\n",
    "        # just duplicate the clip from the known char at the same index\n",
    "        overlay_clips.append( known_char_clips[len(overlay_clips)])\n",
    "        \n",
    "    # other character's clip overlay\n",
    "    else:\n",
    "        overlay_clips.append( other_char_clips_array[rand_index][len(overlay_clips)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eb830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_clips.shape: (1497, 88200)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It should be noted that this only overlays the main character's clip with up to one other clip. \n",
    "It does not currently stack more than that.\n",
    "\"\"\"\n",
    "\n",
    "# the list of arrays to be overlayed (averaged)\n",
    "list_to_overlay = [known_char_clips, overlay_clips]\n",
    "\n",
    "output_clips = np.array(sum(list_to_overlay)/len(list_to_overlay))\n",
    "print('output_clips.shape:', output_clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8292fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TMP TESTING\n",
    "\n",
    "# for i in range(0, 5): \n",
    "#     loc = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/test_overlay'+str(i)+'.wav'\n",
    "#     write(loc, 22050, output_clips[i])\n",
    "    \n",
    "#     # okay, or overlay clips are not generated correctly\n",
    "#     loc2 = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/just_overlay_clip'+str(i)+'.wav'\n",
    "#     write(loc2, 22050, overlay_clips[i])\n",
    "    \n",
    "    \n",
    "# # confirmed they are not all the same\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_4.wav', \n",
    "# #       22050, known_char_clips[4])\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_1004.wav', \n",
    "# #       22050, known_char_clips[1004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13976c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Currently the clips are separated. Let's append them all end-to-end such that the arrays\n",
    "are 1D.\n",
    "\"\"\"\n",
    "\n",
    "# flatten\n",
    "known_char_clips = known_char_clips.flatten()\n",
    "output_clips = output_clips.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3703bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I think we should try to maintain a 70%/15%/15% split for training, validation, and testing data.\n",
    "Toward this end, I think that no clips that appear in one bracket should appear in another, even if\n",
    "we are shuffling things.\n",
    "\n",
    "These proportions can be specified above in the parameter section.\n",
    "\"\"\"\n",
    "\n",
    "# determine clip array slicing\n",
    "end_frame_train = math.ceil(train_proportion * len(output_clips))\n",
    "end_frame_val = end_frame_train + math.ceil(val_proportion * len(output_clips))\n",
    "\n",
    "# slice the known character clips\n",
    "known_char_clips_for_train = known_char_clips[0:end_frame_train]\n",
    "known_char_clips_for_val = known_char_clips[end_frame_train:end_frame_val]\n",
    "known_char_clips_for_test = known_char_clips[end_frame_val:]\n",
    "\n",
    "# slice the output clips\n",
    "output_clips_for_train = output_clips[0:end_frame_train]\n",
    "output_clips_for_val = output_clips[end_frame_train:end_frame_val]\n",
    "output_clips_for_test = output_clips[end_frame_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5caa208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training frames: 92424780 (0 through 92424779) \n",
      "Validation frames: 19805310 (92424780 through 112230089) \n",
      "Test frames: 19805310 (112230090 through 132035400)\n",
      "\n",
      "Training proportion: 92424780/132035400 = 0.700000 \n",
      "Validation proportion: 19805310/132035400 = 0.150000 \n",
      "Testing proportion: 19805310/132035400 = 0.150000\n"
     ]
    }
   ],
   "source": [
    "print('Training frames: %i (0 through %i) \\nValidation frames: %i (%i through %i) \\nTest frames: %i (%i through %i)'\n",
    "     % (len(output_clips_for_train), end_frame_train-1, len(output_clips_for_val), end_frame_train, end_frame_val-1,\n",
    "       len(output_clips_for_test), end_frame_val, len(output_clips)))\n",
    "\n",
    "print()\n",
    "print('Training proportion: %i/%i = %f \\nValidation proportion: %i/%i = %f \\nTesting proportion: %i/%i = %f'\n",
    "     % (len(output_clips_for_train), len(output_clips), len(output_clips_for_train)/len(output_clips),\n",
    "        len(output_clips_for_val), len(output_clips), len(output_clips_for_val)/len(output_clips),\n",
    "        len(output_clips_for_test), len(output_clips), len(output_clips_for_test)/len(output_clips)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "184ab6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the six arrays to file in the form of a dict in the output directory. These are the overlapping audio clips.\n",
    "\"\"\"\n",
    "\n",
    "names_to_save = ['train_input', 'val_input', 'test_input', 'train_labels', 'val_labels', 'test_labels']\n",
    "arrays_to_save = [output_clips_for_train, output_clips_for_val, output_clips_for_test,\n",
    "                  known_char_clips_for_train, known_char_clips_for_val, known_char_clips_for_test]\n",
    "\n",
    "# prepare the audio arrays for storing to file\n",
    "audio_output_dict = {}\n",
    "for index in range(0, len(names_to_save)):\n",
    "    audio_output_dict[names_to_save[index]] = arrays_to_save[index]   \n",
    "\n",
    "# save the audio dict to file\n",
    "with open(wd+'output/audio_training_data_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(audio_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "837d1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_output_dict = {}\n",
    "\n",
    "# generate MFCCs\n",
    "for index in range(0, len(arrays_to_save)):\n",
    "#     tmp_list = []\n",
    "    \n",
    "#     for clip in arrays_to_save[index]:\n",
    "#         tmp_list.append(librosa.feature.mfcc(\n",
    "#             y=clip, \n",
    "#             sr=sr,\n",
    "#             S=None, \n",
    "#             n_mfcc=n_mfcc, \n",
    "#             dct_type=2, \n",
    "#             norm='ortho', \n",
    "#             lifter=0, \n",
    "#             win_length=win_length, # default is 2048; with sr of 22050 audio frames/sec, this corresponds to ~93ms\n",
    "#             hop_length=hop_length  # default is 512; with sr of 22050 audio frames/sec, this corresponds to ~23ms\n",
    "#         ))\n",
    "                \n",
    "    mfcc_array = librosa.feature.mfcc(\n",
    "            y=arrays_to_save[index], \n",
    "            sr=sr,\n",
    "            S=None, \n",
    "            n_mfcc=n_mfcc, \n",
    "            dct_type=2, \n",
    "            norm='ortho', \n",
    "            lifter=0, \n",
    "            win_length=win_length, # default is 2048; with sr of 22050 audio frames/sec, this corresponds to ~93ms\n",
    "            hop_length=hop_length  # default is 512; with sr of 22050 audio frames/sec, this corresponds to ~23ms\n",
    "        )\n",
    "    mfcc_output_dict[names_to_save[index]] = np.swapaxes(np.array(mfcc_array), 0, 1)\n",
    "    \n",
    "        \n",
    "# save the mfcc dict to file\n",
    "with open(wd+'output/mfcc_training_data_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(mfcc_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "82f3cfe2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "audio_training_data_dict.shape: (92424780,)\n",
      "mfcc_training_data_dict.shape: (180518, 64)\n"
     ]
    }
   ],
   "source": [
    "with open(wd+'output/audio_training_data_dict.pickle', 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    print('audio_training_data_dict.shape:', reloaded_dict.get('train_input').shape)\n",
    "\n",
    "with open(wd+'output/mfcc_training_data_dict.pickle', 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    print('mfcc_training_data_dict.shape:', reloaded_dict.get('train_input').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e975e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
