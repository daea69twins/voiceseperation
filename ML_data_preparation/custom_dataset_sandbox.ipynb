{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596f595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The goal is to take the original code from the asteroid library for an inherited, custom Dataset object \n",
    "and fit it to our needs.\n",
    "\n",
    "Original code:\n",
    "https://github.com/daea69twins/voiceseperation/wiki/Time-Log/_edit\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile as sf\n",
    "import torch\n",
    "from torch import hub\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random as random\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "\n",
    "from .wham_dataset import wham_noise_license\n",
    "\n",
    "MINI_URL = \"https://zenodo.org/record/3871592/files/MiniLibriMix.zip?download=1\"\n",
    "\n",
    "\n",
    "class LibriMix(Dataset):\n",
    "    \"\"\"Dataset class for LibriMix source separation tasks.\n",
    "\n",
    "    Args:\n",
    "        csv_dir (str): The path to the metadata file.\n",
    "        task (str): One of ``'enh_single'``, ``'enh_both'``, ``'sep_clean'`` or\n",
    "            ``'sep_noisy'`` :\n",
    "\n",
    "            * ``'enh_single'`` for single speaker speech enhancement.\n",
    "            * ``'enh_both'`` for multi speaker speech enhancement.\n",
    "            * ``'sep_clean'`` for two-speaker clean source separation.\n",
    "            * ``'sep_noisy'`` for two-speaker noisy source separation.\n",
    "\n",
    "        sample_rate (int) : The sample rate of the sources and mixtures.\n",
    "        n_src (int) : The number of sources in the mixture.\n",
    "        segment (int, optional) : The desired sources and mixtures length in s.\n",
    "\n",
    "    References\n",
    "        [1] \"LibriMix: An Open-Source Dataset for Generalizable Speech Separation\",\n",
    "        Cosentino et al. 2020.\n",
    "    \"\"\"\n",
    "\n",
    "    dataset_name = \"LibriMix\"\n",
    "\n",
    "    def __init__(\n",
    "        self, csv_dir, task=\"sep_clean\", sample_rate=16000, n_src=2, segment=3, return_id=False\n",
    "    ):\n",
    "        self.csv_dir = csv_dir\n",
    "        self.task = task\n",
    "        self.return_id = return_id\n",
    "        # Get the csv corresponding to the task\n",
    "        if task == \"enh_single\":\n",
    "            md_file = [f for f in os.listdir(csv_dir) if \"single\" in f][0]\n",
    "            self.csv_path = os.path.join(self.csv_dir, md_file)\n",
    "        elif task == \"enh_both\":\n",
    "            md_file = [f for f in os.listdir(csv_dir) if \"both\" in f][0]\n",
    "            self.csv_path = os.path.join(self.csv_dir, md_file)\n",
    "            md_clean_file = [f for f in os.listdir(csv_dir) if \"clean\" in f][0]\n",
    "            self.df_clean = pd.read_csv(os.path.join(csv_dir, md_clean_file))\n",
    "        elif task == \"sep_clean\":\n",
    "            md_file = [f for f in os.listdir(csv_dir) if \"clean\" in f][0]\n",
    "            self.csv_path = os.path.join(self.csv_dir, md_file)\n",
    "        elif task == \"sep_noisy\":\n",
    "            md_file = [f for f in os.listdir(csv_dir) if \"both\" in f][0]\n",
    "            self.csv_path = os.path.join(self.csv_dir, md_file)\n",
    "        self.segment = segment\n",
    "        self.sample_rate = sample_rate\n",
    "        # Open csv file\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        # Get rid of the utterances too short\n",
    "        if self.segment is not None:\n",
    "            max_len = len(self.df)\n",
    "            self.seg_len = int(self.segment * self.sample_rate)\n",
    "            # Ignore the file shorter than the desired_length\n",
    "            self.df = self.df[self.df[\"length\"] >= self.seg_len]\n",
    "            print(\n",
    "                f\"Drop {max_len - len(self.df)} utterances from {max_len} \"\n",
    "                f\"(shorter than {segment} seconds)\"\n",
    "            )\n",
    "        else:\n",
    "            self.seg_len = None\n",
    "        self.n_src = n_src\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row in dataframe\n",
    "        row = self.df.iloc[idx]\n",
    "        # Get mixture path\n",
    "        mixture_path = row[\"mixture_path\"]\n",
    "        self.mixture_path = mixture_path\n",
    "        sources_list = []\n",
    "        # If there is a seg start point is set randomly\n",
    "        if self.seg_len is not None:\n",
    "            start = random.randint(0, row[\"length\"] - self.seg_len)\n",
    "            stop = start + self.seg_len\n",
    "        else:\n",
    "            start = 0\n",
    "            stop = None\n",
    "        # If task is enh_both then the source is the clean mixture\n",
    "        if \"enh_both\" in self.task:\n",
    "            mix_clean_path = self.df_clean.iloc[idx][\"mixture_path\"]\n",
    "            s, _ = sf.read(mix_clean_path, dtype=\"float32\", start=start, stop=stop)\n",
    "            sources_list.append(s)\n",
    "\n",
    "        else:\n",
    "            # Read sources\n",
    "            for i in range(self.n_src):\n",
    "                source_path = row[f\"source_{i + 1}_path\"]\n",
    "                s, _ = sf.read(source_path, dtype=\"float32\", start=start, stop=stop)\n",
    "                sources_list.append(s)\n",
    "        # Read the mixture\n",
    "        mixture, _ = sf.read(mixture_path, dtype=\"float32\", start=start, stop=stop)\n",
    "        # Convert to torch tensor\n",
    "        mixture = torch.from_numpy(mixture)\n",
    "        # Stack sources\n",
    "        sources = np.vstack(sources_list)\n",
    "        # Convert sources to tensor\n",
    "        sources = torch.from_numpy(sources)\n",
    "        if not self.return_id:\n",
    "            return mixture, sources\n",
    "        # 5400-34479-0005_4973-24515-0007.wav\n",
    "        id1, id2 = mixture_path.split(\"/\")[-1].split(\".\")[0].split(\"_\")\n",
    "        return mixture, sources, [id1, id2]\n",
    "\n",
    "    @classmethod\n",
    "    def loaders_from_mini(cls, batch_size=4, **kwargs):\n",
    "        \"\"\"Downloads MiniLibriMix and returns train and validation DataLoader.\n",
    "\n",
    "        Args:\n",
    "            batch_size (int): Batch size of the Dataloader. Only DataLoader param.\n",
    "                To have more control on Dataloader, call `mini_from_download` and\n",
    "                instantiate the DatalLoader.\n",
    "            **kwargs: keyword arguments to pass the `LibriMix`, see `__init__`.\n",
    "                The kwargs will be fed to both the training set and validation\n",
    "                set.\n",
    "\n",
    "        Returns:\n",
    "            train_loader, val_loader: training and validation DataLoader out of\n",
    "            `LibriMix` Dataset.\n",
    "\n",
    "        Examples\n",
    "            >>> from asteroid.data import LibriMix\n",
    "            >>> train_loader, val_loader = LibriMix.loaders_from_mini(\n",
    "            >>>     task='sep_clean', batch_size=4\n",
    "            >>> )\n",
    "        \"\"\"\n",
    "        train_set, val_set = cls.mini_from_download(**kwargs)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, drop_last=True)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, drop_last=True)\n",
    "        return train_loader, val_loader\n",
    "\n",
    "    @classmethod\n",
    "    def mini_from_download(cls, **kwargs):\n",
    "        \"\"\"Downloads MiniLibriMix and returns train and validation Dataset.\n",
    "        If you want to instantiate the Dataset by yourself, call\n",
    "        `mini_download` that returns the path to the path to the metadata files.\n",
    "\n",
    "        Args:\n",
    "            **kwargs: keyword arguments to pass the `LibriMix`, see `__init__`.\n",
    "                The kwargs will be fed to both the training set and validation\n",
    "                set\n",
    "\n",
    "        Returns:\n",
    "            train_set, val_set: training and validation instances of\n",
    "            `LibriMix` (data.Dataset).\n",
    "\n",
    "        Examples\n",
    "            >>> from asteroid.data import LibriMix\n",
    "            >>> train_set, val_set = LibriMix.mini_from_download(task='sep_clean')\n",
    "        \"\"\"\n",
    "        # kwargs checks\n",
    "        assert \"csv_dir\" not in kwargs, \"Cannot specify csv_dir when downloading.\"\n",
    "        assert kwargs.get(\"task\", \"sep_clean\") in [\n",
    "            \"sep_clean\",\n",
    "            \"sep_noisy\",\n",
    "        ], \"Only clean and noisy separation are supported in MiniLibriMix.\"\n",
    "        assert (\n",
    "            kwargs.get(\"sample_rate\", 8000) == 8000\n",
    "        ), \"Only 8kHz sample rate is supported in MiniLibriMix.\"\n",
    "        # Download LibriMix in current directory\n",
    "        meta_path = cls.mini_download()\n",
    "        # Create dataset instances\n",
    "        train_set = cls(os.path.join(meta_path, \"train\"), sample_rate=8000, **kwargs)\n",
    "        val_set = cls(os.path.join(meta_path, \"val\"), sample_rate=8000, **kwargs)\n",
    "        return train_set, val_set\n",
    "\n",
    "    @staticmethod\n",
    "    def mini_download():\n",
    "        \"\"\"Downloads MiniLibriMix from Zenodo in current directory\n",
    "\n",
    "        Returns:\n",
    "            The path to the metadata directory.\n",
    "        \"\"\"\n",
    "        mini_dir = \"./MiniLibriMix/\"\n",
    "        os.makedirs(mini_dir, exist_ok=True)\n",
    "        # Download zip (or cached)\n",
    "        zip_path = mini_dir + \"MiniLibriMix.zip\"\n",
    "        if not os.path.isfile(zip_path):\n",
    "            hub.download_url_to_file(MINI_URL, zip_path)\n",
    "        # Unzip zip\n",
    "        cond = all([os.path.isdir(\"MiniLibriMix/\" + f) for f in [\"train\", \"val\", \"metadata\"]])\n",
    "        if not cond:\n",
    "            with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "                zip_ref.extractall(\"./\")  # Will unzip in MiniLibriMix\n",
    "        # Reorder metadata\n",
    "        src = \"MiniLibriMix/metadata/\"\n",
    "        for mode in [\"train\", \"val\"]:\n",
    "            dst = f\"MiniLibriMix/metadata/{mode}/\"\n",
    "            os.makedirs(dst, exist_ok=True)\n",
    "            [\n",
    "                shutil.copyfile(src + f, dst + f)\n",
    "                for f in os.listdir(src)\n",
    "                if mode in f and os.path.isfile(src + f)\n",
    "            ]\n",
    "        return \"./MiniLibriMix/metadata\"\n",
    "\n",
    "    def get_infos(self):\n",
    "        \"\"\"Get dataset infos (for publishing models).\n",
    "\n",
    "        Returns:\n",
    "            dict, dataset infos with keys `dataset`, `task` and `licences`.\n",
    "        \"\"\"\n",
    "        infos = dict()\n",
    "        infos[\"dataset\"] = self._dataset_name()\n",
    "        infos[\"task\"] = self.task\n",
    "        if self.task == \"sep_clean\":\n",
    "            data_license = [librispeech_license]\n",
    "        else:\n",
    "            data_license = [librispeech_license, wham_noise_license]\n",
    "        infos[\"licenses\"] = data_license\n",
    "        return infos\n",
    "\n",
    "    def _dataset_name(self):\n",
    "        \"\"\" Differentiate between 2 and 3 sources.\"\"\"\n",
    "        return f\"Libri{self.n_src}Mix\"\n",
    "\n",
    "\n",
    "librispeech_license = dict(\n",
    "    title=\"LibriSpeech ASR corpus\",\n",
    "    title_link=\"http://www.openslr.org/12\",\n",
    "    author=\"Vassil Panayotov\",\n",
    "    author_link=\"https://github.com/vdp\",\n",
    "    license=\"CC BY 4.0\",\n",
    "    license_link=\"https://creativecommons.org/licenses/by/4.0/\",\n",
    "    non_commercial=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
