{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b874b2f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ZaknafeinII\\anaconda3\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "To use this script, make the following preparations:\n",
    "- Create a working directory\n",
    "- Inside the working directory, have a single ..._clips.npy file. This file will correspond to which\n",
    "  character's model the generated data will be used to train.\n",
    "- Inside the working directory, have a single folder named 'overlay_clips'\n",
    "- Inside the overlay_clips directory, include one or more ..._clips.npy files of other characters. This will\n",
    "  be used generate data for the known character's model to differentiate from other characters.\n",
    "- Inside the working directory, create an empty subdirectory named 'output'\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pydub\n",
    "import random\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a77966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify script parameters\n",
    "\"\"\"\n",
    "# specify the working directory; note that directory notation must use '/' rather than '\\'\n",
    "wd = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/'\n",
    "\n",
    "# initially using 70%/15%/15% proportions for training, validation, and testing\n",
    "train_proportion = 0.7\n",
    "val_proportion = 0.15\n",
    "\n",
    "# number of MFCC features\n",
    "# 20 is the default for the librosa .wav->MFCC feature extraction\n",
    "# 39 seems to be normal for speech recognition (but not reconstruction)\n",
    "# 64 seems to be the minimum for good speech reconstruction, but the reconstruction can be lengthy\n",
    "n_mfcc = 64\n",
    "\n",
    "# MFCC framing of audio frames\n",
    "# 2048 and 512 are the default values, respectively\n",
    "win_length=2048\n",
    "hop_length=512\n",
    "\n",
    "# the sampling rate to use; default is 22050\n",
    "sr = 22050"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dca2934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For known character data: id10016_merged_cleaned.wav_clips.npy\n",
      "\n",
      "For other character data:\n",
      "\tid10130_merged_cleaned.wav_clips.npy\n",
      "\tid10168_merged_cleaned.wav_clips.npy\n",
      "\tid10484_merged_cleaned.wav_clips.npy\n"
     ]
    }
   ],
   "source": [
    "# this will store the known character's clips\n",
    "known_char_clips = None\n",
    "\n",
    "# this will store the clips for all other characters\n",
    "other_char_clips_array = []\n",
    "\n",
    "# navigate the specified working directory to determine known character and other character clips arrays\n",
    "wd_contents = os.listdir(wd)\n",
    "wd_overlays_contents = os.listdir(wd+'overlay_clips')\n",
    "\n",
    "for item in wd_contents:\n",
    "    if os.path.isfile(os.path.join(wd, item)):\n",
    "        print('For known character data: %s\\n' % (item))\n",
    "        \n",
    "        # load the known character's ...clips.npy file\n",
    "        with open(wd+item, 'rb') as f:\n",
    "            known_char_clips = np.load(f)\n",
    "            \n",
    "print('For other character data:')\n",
    "for item in wd_overlays_contents:\n",
    "    # load the current other character's ...clips.npy file\n",
    "        with open(wd+'overlay_clips/'+item, 'rb') as f:\n",
    "            other_char_clips_array.append(np.load(f))\n",
    "            \n",
    "        print('\\t%s' % (item))\n",
    "    \n",
    "            \n",
    "other_char_clips_array = np.array(other_char_clips_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4646ffc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "known_char_clips.shape: (1497, 88200)\n",
      "other_char_clips_array.shape: (3, 1497, 88200)\n"
     ]
    }
   ],
   "source": [
    "print('known_char_clips.shape:', known_char_clips.shape)\n",
    "print('other_char_clips_array.shape:', other_char_clips_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b6a3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffle all clips for all characters and set up the clips to be overlayed.\n",
    "Because clip overlaying is based on averaging values, overlaying a known char clip\n",
    "with itself returns itself.\n",
    "\n",
    "There are n+1 overlay options that are selected randomly, where n is the number of\n",
    "other characters to choose from. The +1 is the choice of no overlay (by overlaying with\n",
    "itself).\n",
    "\"\"\"\n",
    "\n",
    "# we need to shuffle order of the clips\n",
    "# shuffle known character\n",
    "random.shuffle(known_char_clips)\n",
    "# shuffle other characters\n",
    "for index in range(0, len(other_char_clips_array)):\n",
    "    random.shuffle(other_char_clips_array[index])\n",
    "    \n",
    "\n",
    "# overlay_clips stores the clips to overlay the known character\n",
    "overlay_clips = []\n",
    "\n",
    "# randomly select clips from the other characters to append to overlay clips\n",
    "# can also do the same clip as the known character (which represents no overlay, just the normal voice)\n",
    "# all scenarios have equal chance\n",
    "while len(overlay_clips) < len(known_char_clips):\n",
    "    # determine random index\n",
    "    # randint() is inclusive; if the int is out of bounds we interpret this as no overlay\n",
    "    rand_index = random.randint(0, len(other_char_clips_array))\n",
    "    \n",
    "    # no overlay\n",
    "    if rand_index == len(other_char_clips_array):\n",
    "        # just duplicate the clip from the known char at the same index\n",
    "        overlay_clips.append( known_char_clips[len(overlay_clips)])\n",
    "        \n",
    "    # other character's clip overlay\n",
    "    else:\n",
    "        overlay_clips.append( other_char_clips_array[rand_index][len(overlay_clips)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6eb830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_clips.shape: (1497, 88200)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "It should be noted that this only overlays the main character's clip with up to one other clip. \n",
    "It does not currently stack more than that.\n",
    "\"\"\"\n",
    "\n",
    "# the list of arrays to be overlayed (averaged)\n",
    "list_to_overlay = [known_char_clips, overlay_clips]\n",
    "\n",
    "output_clips = np.array(sum(list_to_overlay)/len(list_to_overlay))\n",
    "print('output_clips.shape:', output_clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8292fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TMP TESTING\n",
    "\n",
    "# for i in range(0, 5): \n",
    "#     loc = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/test_overlay'+str(i)+'.wav'\n",
    "#     write(loc, 22050, output_clips[i])\n",
    "    \n",
    "#     # okay, or overlay clips are not generated correctly\n",
    "#     loc2 = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/just_overlay_clip'+str(i)+'.wav'\n",
    "#     write(loc2, 22050, overlay_clips[i])\n",
    "    \n",
    "    \n",
    "# # confirmed they are not all the same\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_4.wav', \n",
    "# #       22050, known_char_clips[4])\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_1004.wav', \n",
    "# #       22050, known_char_clips[1004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e3703bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I think we should try to maintain a 70%/15%/15% split for training, validation, and testing data.\n",
    "Toward this end, I think that no clips that appear in one bracket should appear in another, even if\n",
    "we are shuffling things.\n",
    "\n",
    "These proportions can be specified above in the parameter section.\n",
    "\"\"\"\n",
    "\n",
    "# determine clip array slicing\n",
    "end_frame_train = math.ceil(train_proportion * len(output_clips))\n",
    "end_frame_val = end_frame_train + math.ceil(val_proportion * len(output_clips))\n",
    "\n",
    "# slice the known character clips\n",
    "known_char_clips_for_train = known_char_clips[0:end_frame_train]\n",
    "known_char_clips_for_val = known_char_clips[end_frame_train:end_frame_val]\n",
    "known_char_clips_for_test = known_char_clips[end_frame_val:]\n",
    "\n",
    "# slice the output clips\n",
    "output_clips_for_train = output_clips[0:end_frame_train]\n",
    "output_clips_for_val = output_clips[end_frame_train:end_frame_val]\n",
    "output_clips_for_test = output_clips[end_frame_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5caa208f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training frames: 1048 (0 through 1047) \n",
      "Validation frames: 225 (1048 through 1272) \n",
      "Test frames: 224 (1273 through 1497)\n",
      "\n",
      "Training proportion: 1048/1497 = 0.700067 \n",
      "Validation proportion: 225/1497 = 0.150301 \n",
      "Testing proportion: 224/1497 = 0.149633\n"
     ]
    }
   ],
   "source": [
    "print('Training frames: %i (0 through %i) \\nValidation frames: %i (%i through %i) \\nTest frames: %i (%i through %i)'\n",
    "     % (len(output_clips_for_train), end_frame_train-1, len(output_clips_for_val), end_frame_train, end_frame_val-1,\n",
    "       len(output_clips_for_test), end_frame_val, len(output_clips)))\n",
    "\n",
    "print()\n",
    "print('Training proportion: %i/%i = %f \\nValidation proportion: %i/%i = %f \\nTesting proportion: %i/%i = %f'\n",
    "     % (len(output_clips_for_train), len(output_clips), len(output_clips_for_train)/len(output_clips),\n",
    "        len(output_clips_for_val), len(output_clips), len(output_clips_for_val)/len(output_clips),\n",
    "        len(output_clips_for_test), len(output_clips), len(output_clips_for_test)/len(output_clips)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "184ab6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParameterError",
     "evalue": "Invalid shape for monophonic audio: ndim=2, shape=(1048, 88200)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ZAKNAF~1\\AppData\\Local\\Temp/ipykernel_16820/3423343623.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mmfcc_output_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mindex\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames_to_save\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m     mfcc_output_dict[names_to_save[index]] = data = librosa.feature.mfcc(\n\u001b[0m\u001b[0;32m     18\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marrays_to_save\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\feature\\spectral.py\u001b[0m in \u001b[0;36mmfcc\u001b[1;34m(y, sr, S, n_mfcc, dct_type, norm, lifter, **kwargs)\u001b[0m\n\u001b[0;32m   1849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mS\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1851\u001b[1;33m         \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpower_to_db\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmelspectrogram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1852\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1853\u001b[0m     \u001b[0mM\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfftpack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdct_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_mfcc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\feature\\spectral.py\u001b[0m in \u001b[0;36mmelspectrogram\u001b[1;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[0;32m   1993\u001b[0m     \"\"\"\n\u001b[0;32m   1994\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1995\u001b[1;33m     S, n_fft = _spectrogram(\n\u001b[0m\u001b[0;32m   1996\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m         \u001b[0mS\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mS\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py\u001b[0m in \u001b[0;36m_spectrogram\u001b[1;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[0;32m   2510\u001b[0m         S = (\n\u001b[0;32m   2511\u001b[0m             np.abs(\n\u001b[1;32m-> 2512\u001b[1;33m                 stft(\n\u001b[0m\u001b[0;32m   2513\u001b[0m                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2514\u001b[0m                     \u001b[0mn_fft\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_fft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\spectrum.py\u001b[0m in \u001b[0;36mstft\u001b[1;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[1;31m# Check audio is valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid_audio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;31m# Pad the time series so that frames are centered\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\util\\utils.py\u001b[0m in \u001b[0;36mvalid_audio\u001b[1;34m(y, mono)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mmono\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 293\u001b[1;33m         raise ParameterError(\n\u001b[0m\u001b[0;32m    294\u001b[0m             \u001b[1;34m\"Invalid shape for monophonic audio: \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;34m\"ndim={:d}, shape={}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mParameterError\u001b[0m: Invalid shape for monophonic audio: ndim=2, shape=(1048, 88200)"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Save the six arrays to file in the form of a dict in the output directory. These are the overlapping audio clips.\n",
    "\"\"\"\n",
    "\n",
    "names_to_save = ['train_input', 'val_input', 'test_input', 'train_labels', 'val_labels', 'test_labels']\n",
    "arrays_to_save = [output_clips_for_train, output_clips_for_val, output_clips_for_test,\n",
    "                  known_char_clips_for_train, known_char_clips_for_val, known_char_clips_for_test]\n",
    "\n",
    "# prepare the audio arrays for storing to file\n",
    "audio_output_dict = {}\n",
    "for index in range(0, len(names_to_save)):\n",
    "    audio_output_dict[names_to_save[index]] = arrays_to_save[index]   \n",
    "    \n",
    "with open(wd+'output/audio_training_data_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(audio_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d3db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://stackoverflow.com/questions/42492246/how-to-normalize-the-volume-of-an-audio-file-in-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9df880",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wd+'output/audio_training_data_dict.pickle', 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    \n",
    "print(reloaded_dict)\n",
    "\n",
    "with open(wd+'output/mfcc_training_data_dict.pickle', 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    \n",
    "print(reloaded_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert and prepare the MFCC arrays for storing to file\n",
    "mfcc_output_dict = {}\n",
    "for index in range(0, len(names_to_save)):\n",
    "    mfcc_output_dict[names_to_save[index]] = data = librosa.feature.mfcc(\n",
    "        y=arrays_to_save[index], \n",
    "        sr=sr,\n",
    "        S=None, \n",
    "        n_mfcc=n_mfcc, \n",
    "        dct_type=2, \n",
    "        norm='ortho', \n",
    "        lifter=0, \n",
    "        win_length=win_length, # default is 2048; with sr of 22050 audio frames/sec, this corresponds to ~93ms\n",
    "        hop_length=hop_length  # default is 512; with sr of 22050 audio frames/sec, this corresponds to ~23ms\n",
    "    )\n",
    "    \n",
    "    # TODO: look into parameter lifter -- something to do with cepstral coeffiecients\n",
    "    \n",
    "with open(wd+'output/mfcc_training_data_dict.pickle', 'wb') as f:\n",
    "    pickle.dump(mfcc_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
