{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b874b2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "To use this script, make the following preparations:\n",
    "- Create a working directory\n",
    "- Inside the working directory, have a single ..._clips.npy file. This file will correspond to which\n",
    "  character's model the generated data will be used to train.\n",
    "- Inside the working directory, have a single folder named 'overlay_clips'\n",
    "- Inside the overlay_clips directory, include one or more ..._clips.npy files of other characters. This will\n",
    "  be used generate data for the known character's model to differentiate from other characters.\n",
    "- Inside the working directory, create an empty subdirectory named 'output'\n",
    "\n",
    "\n",
    "03/26/22\n",
    "Let's split the known character's training data into 4 second clips with 0.5 second jumps inbetween. \n",
    "We can reuse each of these 4 second clips 4 times?\n",
    "\n",
    "Then, we randomly select one of the overlay clips, and then randomly select a 4 second segment from that clip,\n",
    "overlaying that with the 4 second clip from the known character.\n",
    "\n",
    "This time, instead of pickling everything, we need to store it as raw .wav files and iteratively construct\n",
    "a metadata file (.csv I think?).\n",
    "We will need different metadata for training, validation, and testing.\n",
    "I am unsure how to split these. I think we will generate all audio, and then randomly determine which goes where.\n",
    "I think we can have three folders, \"overlay\", \"char\", and \"noise\", and then the three metadata files can use\n",
    "the same folders but access different files.\n",
    "\n",
    "To avoid overloading/wasting system resouces, we cannot open all of the overlay clips at once. We can iterate \n",
    "over the directory to collect their names, and then open and close them as needed.\n",
    "\n",
    "This current method of opening the other character clips one at a time is unfeasible. We need to create a folder\n",
    "for each of our characters, in which there are 4 second clips with some jump between them. This way we can reuse\n",
    "them as a reference. From there, we only need to generate one more folder for each character we want to make a\n",
    "model for, and that is the overlay folder. \n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import librosa\n",
    "import os\n",
    "import pydub\n",
    "import random\n",
    "import math\n",
    "# import pickle\n",
    "\n",
    "from scipy.io.wavfile import write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f5defd",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "total = 4500\n",
    "jump = 4\n",
    "length = 4\n",
    "while total > length:\n",
    "    total = total - length\n",
    "    total = total + length - jump\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a77966de",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Specify script parameters\n",
    "\"\"\"\n",
    "# specify the working directory; note that directory notation must use '/' rather than '\\'\n",
    "wd = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id20/'\n",
    "\n",
    "# initially using 70%/15%/15% proportions for training, validation, and testing\n",
    "train_proportion = 0.7\n",
    "val_proportion = 0.15\n",
    "\n",
    "# the sampling rate to use; default is 22050\n",
    "sr = 22050\n",
    "\n",
    "desired_samples = 100\n",
    "sample_length = 4 # seconds\n",
    "sample_framecount = sr * sample_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will store the known character's clips\n",
    "known_char_wav = None\n",
    "sr = None\n",
    "\n",
    "# this will store the names of the clips for all other characters\n",
    "other_char_clips_array = []\n",
    "\n",
    "# navigate the specified working directory to determine known character and other character clips arrays\n",
    "wd_contents = os.listdir(wd)\n",
    "wd_overlays_contents = os.listdir(wd+'overlay_clips')\n",
    "\n",
    "for item in wd_contents:\n",
    "    if os.path.isfile(os.path.join(wd, item)):\n",
    "        print('For known character data: %s\\n' % (item))\n",
    "        \n",
    "        # load the known character's ...clips.npy file\n",
    "        with open(wd+item, 'rb') as f:\n",
    "            known_char_wav, sr = librosa.load(f)\n",
    "            \n",
    "print('For other character data:')\n",
    "for item in wd_overlays_contents:\n",
    "#         other_char_clips_array.append(wd+'overlay_clips/'+item)\n",
    "        other_char_clips_array.append(librosa.load(wd+'overlay_clips/'+item))\n",
    "            \n",
    "        print('\\t%s' % (item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4646ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = librosa.get_duration(y=known_char_wav, sr=sr)\n",
    "hours = duration // 3600\n",
    "duration = duration % 3600\n",
    "minutes = duration // 60\n",
    "duration = duration % 60\n",
    "print('known_char_wav duration: %i:%i:%i' % (hours, minutes, duration))\n",
    "print('known_char_wav frame count: %i' % (known_char_wav.shape))\n",
    "print('other_char_clips_array length:', len(other_char_clips_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94541231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n",
      "(88200,)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\ZAKNAF~1\\AppData\\Local\\Temp/ipykernel_8908/2075720153.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# randomly determine which other character's .wav file to use\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mrand_wav_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_char_clips_array\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0mother_char_wav\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother_char_clips_array\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrand_wav_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[0mnoise_framecount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mother_char_wav\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    173\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msr\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr_native\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mres_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\librosa\\core\\audio.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, **kwargs)\u001b[0m\n\u001b[0;32m    602\u001b[0m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoxr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 604\u001b[1;33m         \u001b[0my_hat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresampy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_sr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mres_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    605\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    606\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfix\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\resampy\\core.py\u001b[0m in \u001b[0;36mresample\u001b[1;34m(x, sr_orig, sr_new, axis, filter, **kwargs)\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0mx_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[0my_2d\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mswapaxes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m     \u001b[0mresample_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_2d\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_win\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterp_delta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprecision\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "metadata_array = []\n",
    "char_framecount = known_char_wav.shape[0]\n",
    "\n",
    "desired_samples = 2\n",
    "\n",
    "while len(metadata_array) < desired_samples:\n",
    "    # determine the known character's clip\n",
    "    rand_frame = random.randint(0, char_framecount)\n",
    "    \n",
    "    char_clip = None\n",
    "    overrun = rand_frame + sample_framecount - char_framecount\n",
    "    \n",
    "    # slice out the clip for the known character based on the randomly selected starting frame\n",
    "    # if the starting frame is close to the end of the .wav file, loop back to the beginnning\n",
    "    if overrun > 0:\n",
    "        char_clip = np.concatenate((known_char_wav[rand_frame:], known_char_wav[0:overrun]))\n",
    "    else:\n",
    "        char_clip = known_char_wav[rand_frame:rand_frame+sample_framecount]\n",
    "        \n",
    "        \n",
    "        \n",
    "    # randomly determine which other character's .wav file to use\n",
    "    rand_wav_index = random.randint(0, len(other_char_clips_array))\n",
    "    other_char_wav, _ = librosa.load(other_char_clips_array[rand_wav_index])\n",
    "    \n",
    "    noise_framecount = other_char_wav.shape[0]\n",
    "    rand_frame = random.randint(0, noise_framecount)\n",
    "    \n",
    "    noise_clip = None\n",
    "    overrun = rand_frame + sample_framecount - noise_framecount\n",
    "    \n",
    "    if overrun > 0:\n",
    "        noise_clip = np.concatenate((other_char_wav[rand_frame:], other_char_wav[0:overrun]))\n",
    "    else:\n",
    "        noise_clip = other_char_wav[rand_frame:rand_frame+sample_framecount]\n",
    "        \n",
    "    print(char_clip.shape)\n",
    "    print(noise_clip.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5235dbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## REFERENCE CODE BELOW ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6a3ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Shuffle all clips for all characters and set up the clips to be overlayed.\n",
    "Because clip overlaying is based on averaging values, overlaying a known char clip\n",
    "with itself returns itself.\n",
    "\n",
    "There are n+1 overlay options that are selected randomly, where n is the number of\n",
    "other characters to choose from. The +1 is the choice of no overlay (by overlaying with\n",
    "itself).\n",
    "\"\"\"\n",
    "\n",
    "# we need to shuffle order of the clips\n",
    "# shuffle known character\n",
    "random.shuffle(known_char_clips)\n",
    "# shuffle other characters\n",
    "for index in range(0, len(other_char_clips_array)):\n",
    "    random.shuffle(other_char_clips_array[index])\n",
    "    \n",
    "\n",
    "# overlay_clips stores the clips to overlay the known character\n",
    "overlay_clips = []\n",
    "\n",
    "# randomly select clips from the other characters to append to overlay clips\n",
    "# can also do the same clip as the known character (which represents no overlay, just the normal voice)\n",
    "# all scenarios have equal chance\n",
    "while len(overlay_clips) < len(known_char_clips):\n",
    "    # determine random index\n",
    "    # randint() is inclusive; if the int is out of bounds we interpret this as no overlay\n",
    "    rand_index = random.randint(0, len(other_char_clips_array))\n",
    "    \n",
    "    # no overlay\n",
    "    if rand_index == len(other_char_clips_array):\n",
    "        # just duplicate the clip from the known char at the same index\n",
    "        overlay_clips.append( known_char_clips[len(overlay_clips)])\n",
    "        \n",
    "    # other character's clip overlay\n",
    "    else:\n",
    "        overlay_clips.append( other_char_clips_array[rand_index][len(overlay_clips)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6eb830d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "It should be noted that this only overlays the main character's clip with up to one other clip. \n",
    "It does not currently stack more than that.\n",
    "\"\"\"\n",
    "\n",
    "overlay_clips = np.array(overlay_clips)\n",
    "\n",
    "# the list of arrays to be overlayed (averaged)\n",
    "list_to_overlay = [known_char_clips, overlay_clips]\n",
    "\n",
    "output_clips = np.array(sum(list_to_overlay)/len(list_to_overlay))\n",
    "print('output_clips.shape:', output_clips.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8292fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TMP TESTING\n",
    "\n",
    "# for i in range(0, 5): \n",
    "#     loc = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/test_overlay'+str(i)+'.wav'\n",
    "#     write(loc, 22050, output_clips[i])\n",
    "    \n",
    "#     # okay, or overlay clips are not generated correctly\n",
    "#     loc2 = 'F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/just_overlay_clip'+str(i)+'.wav'\n",
    "#     write(loc2, 22050, overlay_clips[i])\n",
    "    \n",
    "    \n",
    "# # confirmed they are not all the same\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_4.wav', \n",
    "# #       22050, known_char_clips[4])\n",
    "# # write('F:/ZaknafeinII_Backup_02-02-22/daea/training_data_generation/id16/output/known_char_1004.wav', \n",
    "# #       22050, known_char_clips[1004])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13976c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Currently the clips are separated. Let's append them all end-to-end such that the arrays\n",
    "are 1D.\n",
    "\"\"\"\n",
    "\n",
    "# flatten\n",
    "if flatten:\n",
    "    known_char_clips = known_char_clips.flatten()\n",
    "    overlay_clips = overlay_clips.flatten()\n",
    "    output_clips = output_clips.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3703bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I think we should try to maintain a 70%/15%/15% split for training, validation, and testing data.\n",
    "Toward this end, I think that no clips that appear in one bracket should appear in another, even if\n",
    "we are shuffling things.\n",
    "\n",
    "These proportions can be specified above in the parameter section.\n",
    "\"\"\"\n",
    "\n",
    "# determine clip array slicing\n",
    "end_frame_train = math.ceil(train_proportion * len(output_clips))\n",
    "end_frame_val = end_frame_train + math.ceil(val_proportion * len(output_clips))\n",
    "\n",
    "# slice the known character clips\n",
    "known_char_clips_for_train = known_char_clips[0:end_frame_train]\n",
    "known_char_clips_for_val = known_char_clips[end_frame_train:end_frame_val]\n",
    "known_char_clips_for_test = known_char_clips[end_frame_val:]\n",
    "\n",
    "# slice the overlay (noise) clips\n",
    "noise_clips_for_train = overlay_clips[0:end_frame_train]\n",
    "noise_clips_for_val = overlay_clips[end_frame_train:end_frame_val]\n",
    "noise_clips_for_test = overlay_clips[end_frame_val:]\n",
    "\n",
    "# slice the output clips\n",
    "output_clips_for_train = output_clips[0:end_frame_train]\n",
    "output_clips_for_val = output_clips[end_frame_train:end_frame_val]\n",
    "output_clips_for_test = output_clips[end_frame_val:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5caa208f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training frames: %i (0 through %i) \\nValidation frames: %i (%i through %i) \\nTest frames: %i (%i through %i)'\n",
    "     % (len(output_clips_for_train), end_frame_train-1, len(output_clips_for_val), end_frame_train, end_frame_val-1,\n",
    "       len(output_clips_for_test), end_frame_val, len(output_clips)))\n",
    "\n",
    "print()\n",
    "print('Training proportion: %i/%i = %f \\nValidation proportion: %i/%i = %f \\nTesting proportion: %i/%i = %f'\n",
    "     % (len(output_clips_for_train), len(output_clips), len(output_clips_for_train)/len(output_clips),\n",
    "        len(output_clips_for_val), len(output_clips), len(output_clips_for_val)/len(output_clips),\n",
    "        len(output_clips_for_test), len(output_clips), len(output_clips_for_test)/len(output_clips)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184ab6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Save the six arrays to file in the form of a dict in the output directory. These are the overlapping audio clips.\n",
    "\"\"\"\n",
    "\n",
    "names_to_save = ['train_input', 'val_input', 'test_input', 'train_targets', 'val_targets', 'test_targets', \n",
    "                 'train_noise', 'val_noise', 'test_noise']\n",
    "arrays_to_save = [output_clips_for_train, output_clips_for_val, output_clips_for_test,\n",
    "                  known_char_clips_for_train, known_char_clips_for_val, known_char_clips_for_test,\n",
    "                  noise_clips_for_train, noise_clips_for_val, noise_clips_for_test]\n",
    "\n",
    "# prepare the audio arrays for storing to file\n",
    "audio_output_dict = {}\n",
    "for index in range(0, len(names_to_save)):\n",
    "    audio_output_dict[names_to_save[index]] = arrays_to_save[index]   \n",
    "\n",
    "# save the audio dict to file\n",
    "savename = 'flat_' if flatten else ''\n",
    "savename = savename + 'audio_training_data_dict.pickle'\n",
    "with open(wd+'output/'+savename, 'wb') as f:\n",
    "    pickle.dump(audio_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837d1a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mfcc_output_dict = {}\n",
    "\n",
    "# generate MFCCs\n",
    "for index in range(0, len(arrays_to_save)):    \n",
    "    if flatten:\n",
    "        mfcc_array = librosa.feature.mfcc(\n",
    "                y=arrays_to_save[index], \n",
    "                sr=sr,\n",
    "                S=None, \n",
    "                n_mfcc=n_mfcc, \n",
    "                dct_type=2, \n",
    "                norm='ortho', \n",
    "                lifter=0, \n",
    "                win_length=win_length, # default is 2048; with sr of 22050 audio frames/sec, this corresponds to ~93ms\n",
    "                hop_length=hop_length  # default is 512; with sr of 22050 audio frames/sec, this corresponds to ~23ms\n",
    "            )\n",
    "        mfcc_output_dict[names_to_save[index]] = np.array(mfcc_array) # np.swapaxes(np.array(mfcc_array), 0, 1)\n",
    "        \n",
    "    else:\n",
    "        tmp_list = []\n",
    "        \n",
    "        for clip in arrays_to_save[index]:\n",
    "            tmp_list.append(librosa.feature.mfcc(\n",
    "                y=clip, \n",
    "                sr=sr,\n",
    "                S=None, \n",
    "                n_mfcc=n_mfcc, \n",
    "                dct_type=2, \n",
    "                norm='ortho', \n",
    "                lifter=0, \n",
    "                win_length=win_length, # default is 2048; with sr of 22050 audio frames/sec, this corresponds to ~93ms\n",
    "                hop_length=hop_length  # default is 512; with sr of 22050 audio frames/sec, this corresponds to ~23ms\n",
    "            ))\n",
    "            \n",
    "        mfcc_output_dict[names_to_save[index]] = np.array(tmp_list) # np.swapaxes(np.array(tmp_list), 1, 2)\n",
    "    \n",
    "        \n",
    "# save the mfcc dict to file\n",
    "savename = 'flat_' if flatten else ''\n",
    "savename = savename + 'mfcc_training_data_dict.pickle'\n",
    "with open(wd+'output/'+savename, 'wb') as f:\n",
    "    pickle.dump(mfcc_output_dict, f, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f3cfe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(wd+'output/'+savename, 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    print('audio_training_data_dict.shape:', reloaded_dict.get('train_input').shape)\n",
    "    \n",
    "\n",
    "\n",
    "with open(wd+'output/'+savename, 'rb') as f:\n",
    "    reloaded_dict = pickle.load(f)\n",
    "    print('mfcc_training_data_dict.shape:', reloaded_dict.get('train_input').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58e975e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
